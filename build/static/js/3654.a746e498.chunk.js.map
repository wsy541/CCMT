{"version":3,"file":"static/js/3654.a746e498.chunk.js","mappings":"mNAgCA,SAASA,EAAaC,GACpB,IAAMC,EA/BR,SAAmBD,GACjB,IAAME,GAAQC,EAAAA,EAAAA,aAAYH,GAEpBI,EAAS,GACfF,EAAMG,uBACNH,EAAMI,iBAAiB,CACrBC,YADqB,SACTC,EAAIC,EAAIC,EAAIC,GACtBP,EAAOQ,KAAKD,MAKhB,IAAMV,EAASC,EAAMW,eACrBC,QAAQC,IAAId,GACZ,IAAMe,EAAMf,EAAOgB,KAAI,SAAAC,GACrB,IAAMC,EAAOjB,EAAMkB,UAAUF,EAAMC,KAAO,GAE1C,MAAO,CACLE,OAFeC,EAAAA,SAASH,IAASG,EAAAA,SAAAA,oBAGjCC,WAAYL,EAAMP,uBAQtB,OAJAP,EAAOoB,SAAQ,SAAAC,GAAK,OAAIT,EAAIJ,KAAK,CAC/BS,OAAQ,QACRE,WAAYE,OAEPT,EAIQU,CAAU1B,GACzB,MAAO,CACLC,OAAAA,EACA0B,SAAU,IAAIC,GAEjB,IAEKA,EAAAA,WAAAA,SAAAA,KAAAA,EAAAA,EAAAA,GAAAA,KAAAA,GAOH,OAPGA,EAAAA,EAAAA,GAAAA,EAAAA,CAAAA,CAAAA,IAAAA,QAAAA,MACJ,WACE,OAAO,IAAIA,IACZ,oBAED,SAAOC,GACL,OAAO,MACR,EAPGD,GAWOE,EAAb,+EACE,SAASC,EAAMC,GACb,OAAOjC,EAAagC,KAFxB,6BAKE,WACE,OAAO,IAAIH,MANf","sources":["pages/events/add/TokenProvider.ts"],"sourcesContent":["import { createLexer, TokenMap } from './common';\n\nfunction getTokens(input) {\n  const lexer = createLexer(input); // 捕获词法错误\n\n  const errors = [];\n  lexer.removeErrorListeners();\n  lexer.addErrorListener({\n    syntaxError(_1, _2, _3, charPositionInLine) {\n      errors.push(charPositionInLine);\n    }\n\n  }); // 获取token流\n\n  const tokens = lexer.getAllTokens();\n  console.log(tokens);\n  const res = tokens.map(token => {\n    const type = lexer.ruleNames[token.type - 1];\n    const typeName = TokenMap[type] || TokenMap.UnexpectedCharacter;\n    return {\n      scopes: typeName,\n      startIndex: token.charPositionInLine\n    };\n  }); // 将捕获到的错误加入res中\n\n  errors.forEach(point => res.push({\n    scopes: 'error',\n    startIndex: point\n  }));\n  return res;\n}\n\nfunction tokenForLine(input) {\n  const tokens = getTokens(input);\n  return {\n    tokens,\n    endState: new State()\n  };\n}\n\nclass State {\n  clone() {\n    return new State();\n  }\n\n  equals(other) {\n    return true;\n  }\n\n}\n\nexport class TokensProviders {\n  tokenize(line, state) {\n    return tokenForLine(line);\n  }\n\n  getInitialState() {\n    return new State();\n  }\n\n}"],"names":["tokenForLine","input","tokens","lexer","createLexer","errors","removeErrorListeners","addErrorListener","syntaxError","_1","_2","_3","charPositionInLine","push","getAllTokens","console","log","res","map","token","type","ruleNames","scopes","TokenMap","startIndex","forEach","point","getTokens","endState","State","other","TokensProviders","line","state"],"sourceRoot":""}